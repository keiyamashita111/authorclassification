{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GImGbTN3HFYD",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras as K\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "ZJP09jSpHTta",
    "outputId": "31d5fd57-1e2f-429d-e7ba-8bd4b679bd3e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  6,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "       20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37,\n",
       "       38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 50])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "print(tf.__version__)\n",
    "\n",
    "df = pd.read_csv(\"Gungor_2018_VictorianAuthorAttribution_data-train.csv\")\n",
    "\n",
    "df['author'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Y4KUUtRQJUAf",
    "outputId": "caf6c9f3-11fb-42fd-c6b0-383bdaed8d1a"
   },
   "outputs": [],
   "source": [
    "p = np.random.permutation(df['author'].unique())\n",
    "trainclass = p[:40]\n",
    "fewclass = p[40:]\n",
    "\n",
    "trainingclasses = df[df['author'].isin(trainclass)]\n",
    "\n",
    "fewclasses = df[df['author'].isin(fewclass)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVgUDABPELK4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LdNueNb1Ex0H"
   },
   "outputs": [],
   "source": [
    "x = np.array(trainingclasses['text'])\n",
    "y = np.array(pd.factorize(trainingclasses['author'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48472,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n43q4SkwEeJp"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "max_words = 20000\n",
    "                \n",
    "tokenizer = Tokenizer(20000)\n",
    "tokenizer.fit_on_texts(x)\n",
    "sequences = tokenizer.texts_to_sequences(x)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(sequences, y, test_size=0.33, random_state=42)\n",
    "max_review_len = 1000\n",
    "train_x = K.preprocessing.sequence.pad_sequences(train_x,\n",
    "  truncating='pre', padding='pre', maxlen=max_review_len)  # pad and chop!\n",
    "test_x = K.preprocessing.sequence.pad_sequences(test_x,\n",
    "  truncating='pre', padding='pre', maxlen=max_review_len)\n",
    "\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle)\n",
    "    \n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('./GLOVE_DIR', 'glove.6B.50d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 50))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            50,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_review_len,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dbUpfaezHVnT"
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(sequences,\n",
    "                                                    y, test_size=0.33,\n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "viyCWVWjEXbX",
    "outputId": "df1facd0-35dd-44a3-a0ec-ec56c0074e3e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 50)          500050    \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 998, 16)           2416      \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 998, 16)           64        \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 998, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 998, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 498, 16)           784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 498, 16)           64        \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 498, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 498, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 166, 16)           784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_75 (Batc (None, 166, 16)           64        \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 166, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 166, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 55, 16)            784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_76 (Batc (None, 55, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 55, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 55, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 18, 16)            784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_77 (Batc (None, 18, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 18, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 18, 16)            0         \n",
      "_________________________________________________________________\n",
      "features (Flatten)           (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 40)                11560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_78 (Batc (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "main_output (Activation)     (None, 40)                0         \n",
      "=================================================================\n",
      "Total params: 517,642\n",
      "Trainable params: 17,352\n",
      "Non-trainable params: 500,290\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input1 = Input(shape=(1000,))\n",
    "\n",
    "x = embedding_layer(input1)\n",
    "\n",
    "x = Conv1D(16, 3, strides=1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Conv1D(16, 3, strides=2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Conv1D(16, 3, strides=3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Conv1D(16, 3, strides=3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Conv1D(16, 3, strides=3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "feats = Flatten(name='features')(x)\n",
    "\n",
    "x = Dense(40)(feats)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('softmax', name='main_output')(x)\n",
    "\n",
    "model = Model(inputs=input1, outputs=[x, feats])\n",
    "\n",
    "\n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output features missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to features.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "opt = K.optimizers.Adam(lr=0.000001)\n",
    "model.compile(opt, loss={'main_output': 'sparse_categorical_crossentropy'}, metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "colab_type": "code",
    "id": "JV-RM9vC1nqM",
    "outputId": "15136e5d-5b7a-4eec-e7db-6f61d229b5f1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training \n",
      "Train on 32476 samples, validate on 15996 samples\n",
      "Epoch 1/50\n",
      "32476/32476 [==============================] - 5s 162us/step - loss: 1.7053 - main_output_loss: 1.7051 - main_output_acc: 0.5166 - val_loss: 1.7982 - val_main_output_loss: 1.7973 - val_main_output_acc: 0.5021\n",
      "Epoch 2/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.7052 - main_output_loss: 1.7055 - main_output_acc: 0.5187 - val_loss: 1.7976 - val_main_output_loss: 1.7967 - val_main_output_acc: 0.5018\n",
      "Epoch 3/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.7056 - main_output_loss: 1.7052 - main_output_acc: 0.5147 - val_loss: 1.7977 - val_main_output_loss: 1.7968 - val_main_output_acc: 0.5023\n",
      "Epoch 4/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.7018 - main_output_loss: 1.7021 - main_output_acc: 0.5178 - val_loss: 1.7973 - val_main_output_loss: 1.7964 - val_main_output_acc: 0.5021\n",
      "Epoch 5/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.7021 - main_output_loss: 1.7016 - main_output_acc: 0.5172 - val_loss: 1.7970 - val_main_output_loss: 1.7962 - val_main_output_acc: 0.5017\n",
      "Epoch 6/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.7023 - main_output_loss: 1.7022 - main_output_acc: 0.5184 - val_loss: 1.7968 - val_main_output_loss: 1.7959 - val_main_output_acc: 0.5019\n",
      "Epoch 7/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.7014 - main_output_loss: 1.7016 - main_output_acc: 0.5190 - val_loss: 1.7967 - val_main_output_loss: 1.7958 - val_main_output_acc: 0.5014\n",
      "Epoch 8/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.7011 - main_output_loss: 1.7008 - main_output_acc: 0.5168 - val_loss: 1.7959 - val_main_output_loss: 1.7950 - val_main_output_acc: 0.5014\n",
      "Epoch 9/50\n",
      "32476/32476 [==============================] - 3s 94us/step - loss: 1.7011 - main_output_loss: 1.6999 - main_output_acc: 0.5181 - val_loss: 1.7961 - val_main_output_loss: 1.7952 - val_main_output_acc: 0.5018\n",
      "Epoch 10/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.6999 - main_output_loss: 1.7003 - main_output_acc: 0.5193 - val_loss: 1.7964 - val_main_output_loss: 1.7956 - val_main_output_acc: 0.5012\n",
      "Epoch 11/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.6972 - main_output_loss: 1.6978 - main_output_acc: 0.5175 - val_loss: 1.7962 - val_main_output_loss: 1.7953 - val_main_output_acc: 0.5016\n",
      "Epoch 12/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.7000 - main_output_loss: 1.7003 - main_output_acc: 0.5211 - val_loss: 1.7958 - val_main_output_loss: 1.7949 - val_main_output_acc: 0.5012\n",
      "Epoch 13/50\n",
      "32476/32476 [==============================] - 3s 94us/step - loss: 1.6990 - main_output_loss: 1.6988 - main_output_acc: 0.5184 - val_loss: 1.7953 - val_main_output_loss: 1.7944 - val_main_output_acc: 0.5015\n",
      "Epoch 14/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.7014 - main_output_loss: 1.7013 - main_output_acc: 0.5181 - val_loss: 1.7950 - val_main_output_loss: 1.7941 - val_main_output_acc: 0.5019\n",
      "Epoch 15/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.6957 - main_output_loss: 1.6970 - main_output_acc: 0.5188 - val_loss: 1.7951 - val_main_output_loss: 1.7943 - val_main_output_acc: 0.5012\n",
      "Epoch 16/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.6984 - main_output_loss: 1.6984 - main_output_acc: 0.5176 - val_loss: 1.7947 - val_main_output_loss: 1.7939 - val_main_output_acc: 0.5013\n",
      "Epoch 17/50\n",
      "32476/32476 [==============================] - 3s 91us/step - loss: 1.6979 - main_output_loss: 1.6987 - main_output_acc: 0.5171 - val_loss: 1.7939 - val_main_output_loss: 1.7930 - val_main_output_acc: 0.5019\n",
      "Epoch 18/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.6994 - main_output_loss: 1.6992 - main_output_acc: 0.5180 - val_loss: 1.7937 - val_main_output_loss: 1.7929 - val_main_output_acc: 0.5020\n",
      "Epoch 19/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.6942 - main_output_loss: 1.6939 - main_output_acc: 0.5191 - val_loss: 1.7936 - val_main_output_loss: 1.7928 - val_main_output_acc: 0.5016\n",
      "Epoch 20/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.6988 - main_output_loss: 1.6980 - main_output_acc: 0.5194 - val_loss: 1.7935 - val_main_output_loss: 1.7927 - val_main_output_acc: 0.5019\n",
      "Epoch 21/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.6941 - main_output_loss: 1.6937 - main_output_acc: 0.5208 - val_loss: 1.7930 - val_main_output_loss: 1.7922 - val_main_output_acc: 0.5020\n",
      "Epoch 22/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.6990 - main_output_loss: 1.7015 - main_output_acc: 0.5183 - val_loss: 1.7932 - val_main_output_loss: 1.7924 - val_main_output_acc: 0.5019\n",
      "Epoch 23/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.6982 - main_output_loss: 1.6984 - main_output_acc: 0.5202 - val_loss: 1.7935 - val_main_output_loss: 1.7926 - val_main_output_acc: 0.5021\n",
      "Epoch 24/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.7019 - main_output_loss: 1.7035 - main_output_acc: 0.5178 - val_loss: 1.7927 - val_main_output_loss: 1.7918 - val_main_output_acc: 0.5023\n",
      "Epoch 25/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.6962 - main_output_loss: 1.6971 - main_output_acc: 0.5191 - val_loss: 1.7929 - val_main_output_loss: 1.7920 - val_main_output_acc: 0.5026\n",
      "Epoch 26/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.6951 - main_output_loss: 1.6960 - main_output_acc: 0.5240 - val_loss: 1.7925 - val_main_output_loss: 1.7916 - val_main_output_acc: 0.5023\n",
      "Epoch 27/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.6991 - main_output_loss: 1.7000 - main_output_acc: 0.5188 - val_loss: 1.7919 - val_main_output_loss: 1.7911 - val_main_output_acc: 0.5026\n",
      "Epoch 28/50\n",
      "32476/32476 [==============================] - 3s 91us/step - loss: 1.6960 - main_output_loss: 1.6947 - main_output_acc: 0.5213 - val_loss: 1.7922 - val_main_output_loss: 1.7913 - val_main_output_acc: 0.5023\n",
      "Epoch 29/50\n",
      "32476/32476 [==============================] - 3s 91us/step - loss: 1.6957 - main_output_loss: 1.6963 - main_output_acc: 0.5172 - val_loss: 1.7918 - val_main_output_loss: 1.7909 - val_main_output_acc: 0.5024\n",
      "Epoch 30/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.6939 - main_output_loss: 1.6952 - main_output_acc: 0.5210 - val_loss: 1.7913 - val_main_output_loss: 1.7904 - val_main_output_acc: 0.5026\n",
      "Epoch 31/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.7003 - main_output_loss: 1.7007 - main_output_acc: 0.5194 - val_loss: 1.7914 - val_main_output_loss: 1.7905 - val_main_output_acc: 0.5029\n",
      "Epoch 32/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.6947 - main_output_loss: 1.6936 - main_output_acc: 0.5196 - val_loss: 1.7911 - val_main_output_loss: 1.7903 - val_main_output_acc: 0.5026\n",
      "Epoch 33/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.6968 - main_output_loss: 1.6966 - main_output_acc: 0.5173 - val_loss: 1.7907 - val_main_output_loss: 1.7899 - val_main_output_acc: 0.5026\n",
      "Epoch 34/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.6938 - main_output_loss: 1.6940 - main_output_acc: 0.5208 - val_loss: 1.7910 - val_main_output_loss: 1.7902 - val_main_output_acc: 0.5029\n",
      "Epoch 35/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.6907 - main_output_loss: 1.6916 - main_output_acc: 0.5214 - val_loss: 1.7909 - val_main_output_loss: 1.7900 - val_main_output_acc: 0.5026\n",
      "Epoch 36/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.6938 - main_output_loss: 1.6939 - main_output_acc: 0.5216 - val_loss: 1.7911 - val_main_output_loss: 1.7903 - val_main_output_acc: 0.5024\n",
      "Epoch 37/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.6964 - main_output_loss: 1.6958 - main_output_acc: 0.5209 - val_loss: 1.7910 - val_main_output_loss: 1.7901 - val_main_output_acc: 0.5024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.6944 - main_output_loss: 1.6951 - main_output_acc: 0.5199 - val_loss: 1.7903 - val_main_output_loss: 1.7894 - val_main_output_acc: 0.5027\n",
      "Epoch 39/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.6930 - main_output_loss: 1.6940 - main_output_acc: 0.5192 - val_loss: 1.7900 - val_main_output_loss: 1.7892 - val_main_output_acc: 0.5028\n",
      "Epoch 40/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.6977 - main_output_loss: 1.6967 - main_output_acc: 0.5196 - val_loss: 1.7898 - val_main_output_loss: 1.7889 - val_main_output_acc: 0.5026\n",
      "Epoch 41/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.6929 - main_output_loss: 1.6931 - main_output_acc: 0.5189 - val_loss: 1.7902 - val_main_output_loss: 1.7894 - val_main_output_acc: 0.5028\n",
      "Epoch 42/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.6898 - main_output_loss: 1.6899 - main_output_acc: 0.5210 - val_loss: 1.7903 - val_main_output_loss: 1.7895 - val_main_output_acc: 0.5026\n",
      "Epoch 43/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.6937 - main_output_loss: 1.6935 - main_output_acc: 0.5204 - val_loss: 1.7900 - val_main_output_loss: 1.7891 - val_main_output_acc: 0.5032\n",
      "Epoch 44/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.6925 - main_output_loss: 1.6920 - main_output_acc: 0.5218 - val_loss: 1.7898 - val_main_output_loss: 1.7890 - val_main_output_acc: 0.5033\n",
      "Epoch 45/50\n",
      "32476/32476 [==============================] - 3s 91us/step - loss: 1.6889 - main_output_loss: 1.6887 - main_output_acc: 0.5210 - val_loss: 1.7897 - val_main_output_loss: 1.7889 - val_main_output_acc: 0.5030\n",
      "Epoch 46/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.6990 - main_output_loss: 1.6989 - main_output_acc: 0.5186 - val_loss: 1.7891 - val_main_output_loss: 1.7883 - val_main_output_acc: 0.5034\n",
      "Epoch 47/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.6939 - main_output_loss: 1.6947 - main_output_acc: 0.5198 - val_loss: 1.7889 - val_main_output_loss: 1.7880 - val_main_output_acc: 0.5033\n",
      "Epoch 48/50\n",
      "32476/32476 [==============================] - 3s 92us/step - loss: 1.6924 - main_output_loss: 1.6918 - main_output_acc: 0.5210 - val_loss: 1.7887 - val_main_output_loss: 1.7879 - val_main_output_acc: 0.5031\n",
      "Epoch 49/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.6903 - main_output_loss: 1.6903 - main_output_acc: 0.5219 - val_loss: 1.7887 - val_main_output_loss: 1.7879 - val_main_output_acc: 0.5032\n",
      "Epoch 50/50\n",
      "32476/32476 [==============================] - 3s 93us/step - loss: 1.6950 - main_output_loss: 1.6951 - main_output_acc: 0.5166 - val_loss: 1.7890 - val_main_output_loss: 1.7882 - val_main_output_acc: 0.5036\n",
      "Training complete \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. train model\n",
    "bat_size = 512\n",
    "max_epochs = 50\n",
    "print(\"\\nStarting training \")\n",
    "#model.fit(np.array(train_x), train_y, epochs=max_epochs,\n",
    "#  batch_size=bat_size, shuffle=True, verbose=1) \n",
    "model.fit(np.array(train_x), train_y, epochs=max_epochs,\n",
    "  batch_size=bat_size, shuffle=True, verbose=1, validation_data=(np.array(test_x), test_y)) \n",
    "print(\"Training complete \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.array(fewclasses['text'])\n",
    "y2 = np.array(pd.factorize(fewclasses['author'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output features missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to features.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model_fewshot_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = []\n",
    "\n",
    "for i in range(5):\n",
    "    x3.append(np.random.choice(x2[y2==i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1000)\n",
      "(5206,)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(x2.shape)\n",
    "print(x3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = np.array(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(x3)\n",
    "test_x3 = tokenizer.texts_to_sequences(x2)\n",
    "\n",
    "\n",
    "\n",
    "max_review_len = 1000\n",
    "train_x = K.preprocessing.sequence.pad_sequences(sequences,\n",
    "  truncating='pre', padding='pre', maxlen=max_review_len)  # pad and chop!\n",
    "test_x = K.preprocessing.sequence.pad_sequences(test_x3,\n",
    "  truncating='pre', padding='pre', maxlen=max_review_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, _ = model.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, _ = model.predict(np.array(test_x3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.expand_dims(train_features, axis=0)\n",
    "test_features = np.expand_dims(test_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 40)\n",
      "(5206, 1, 40)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif = train_features - test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5206, 5, 40)\n"
     ]
    }
   ],
   "source": [
    "print(dif.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5206,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "distance = np.linalg.norm(dif, axis=2)\n",
    "predictions = np.argmin(distance, axis=1)\n",
    "'''index = np.argpartition(distance, self.k)\n",
    "values = index[:self.k]\n",
    "unique, counts = np.unique(self.y[values], return_counts=True)\n",
    "prediction = unique[np.argmax(counts)]'''\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19996158278908951"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions == y2)/5206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_fewshot_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Few Shot Implementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:author_classification]",
   "language": "python",
   "name": "conda-env-author_classification-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
