{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "M3Gqixx8CfPG",
    "outputId": "66c3bef4-dafd-45a8-9ba6-327fcb66a75c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GImGbTN3HFYD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras as K\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "ZJP09jSpHTta",
    "outputId": "31d5fd57-1e2f-429d-e7ba-8bd4b679bd3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  6,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "       20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37,\n",
       "       38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 50])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "print(tf.__version__)\n",
    "\n",
    "df = pd.read_csv(\"Gungor_2018_VictorianAuthorAttribution_data-train.csv\")\n",
    "\n",
    "df['author'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Y4KUUtRQJUAf",
    "outputId": "caf6c9f3-11fb-42fd-c6b0-383bdaed8d1a"
   },
   "outputs": [],
   "source": [
    "p = np.random.permutation(df['author'].unique())\n",
    "trainclass = p[:40]\n",
    "fewclass = p[40:]\n",
    "\n",
    "trainingclasses = df[df['author'].isin(trainclass)]\n",
    "\n",
    "fewclasses = df[df['author'].isin(fewclass)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVgUDABPELK4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LdNueNb1Ex0H"
   },
   "outputs": [],
   "source": [
    "x = np.array(trainingclasses['text'])\n",
    "y = np.array(pd.factorize(trainingclasses['author'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48472,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n43q4SkwEeJp"
   },
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "                \n",
    "tokenizer = Tokenizer(20000)\n",
    "tokenizer.fit_on_texts(x)\n",
    "sequences = tokenizer.texts_to_sequences(x)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(sequences, y, test_size=0.33, random_state=42)\n",
    "max_review_len = 1000\n",
    "train_x = K.preprocessing.sequence.pad_sequences(train_x,\n",
    "  truncating='pre', padding='pre', maxlen=max_review_len)  # pad and chop!\n",
    "test_x = K.preprocessing.sequence.pad_sequences(test_x,\n",
    "  truncating='pre', padding='pre', maxlen=max_review_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dbUpfaezHVnT"
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(sequences,\n",
    "                                                    y, test_size=0.33,\n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "viyCWVWjEXbX",
    "outputId": "df1facd0-35dd-44a3-a0ec-ec56c0074e3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 8)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 998, 16)           400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 998, 16)           64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 998, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 998, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 498, 16)           784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 498, 16)           64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 498, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 498, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 166, 16)           784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 166, 16)           64        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 166, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 166, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 55, 16)            784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 55, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 55, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 55, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 18, 16)            784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 18, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 18, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 18, 16)            0         \n",
      "_________________________________________________________________\n",
      "features (Flatten)           (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40)                11560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "main_output (Activation)     (None, 40)                0         \n",
      "=================================================================\n",
      "Total params: 175,576\n",
      "Trainable params: 175,336\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output features missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to features.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "input1 = Input(shape=(1000,))\n",
    "\n",
    "x = Embedding(20000, 8, input_length=1000)(input1)\n",
    "\n",
    "x = Conv1D(16, 3, strides=1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Conv1D(16, 3, strides=2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Conv1D(16, 3, strides=3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Conv1D(16, 3, strides=3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Conv1D(16, 3, strides=3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "feats = Flatten(name='features')(x)\n",
    "\n",
    "x = Dense(40)(feats)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('softmax', name='main_output')(x)\n",
    "\n",
    "model = Model(inputs=input1, outputs=[x, feats])\n",
    "\n",
    "\n",
    "opt = K.optimizers.Adam(lr=0.001)\n",
    "model.compile(opt, loss={'main_output': 'sparse_categorical_crossentropy'}, metrics=['acc'])\n",
    "\n",
    "\n",
    "\n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33499, 1000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "colab_type": "code",
    "id": "JV-RM9vC1nqM",
    "outputId": "15136e5d-5b7a-4eec-e7db-6f61d229b5f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training \n",
      "Train on 33499 samples, validate on 16500 samples\n",
      "Epoch 1/50\n",
      "33499/33499 [==============================] - 4s 114us/step - loss: 2.6509 - main_output_loss: 2.6508 - main_output_acc: 0.2921 - val_loss: 2.8321 - val_main_output_loss: 2.8322 - val_main_output_acc: 0.2736\n",
      "Epoch 2/50\n",
      "33499/33499 [==============================] - 4s 112us/step - loss: 2.5179 - main_output_loss: 2.5178 - main_output_acc: 0.3223 - val_loss: 2.5047 - val_main_output_loss: 2.5048 - val_main_output_acc: 0.3065\n",
      "Epoch 3/50\n",
      "33499/33499 [==============================] - 4s 109us/step - loss: 2.3608 - main_output_loss: 2.3607 - main_output_acc: 0.3623 - val_loss: 2.3525 - val_main_output_loss: 2.3525 - val_main_output_acc: 0.3544\n",
      "Epoch 4/50\n",
      "33499/33499 [==============================] - 4s 114us/step - loss: 2.2523 - main_output_loss: 2.2521 - main_output_acc: 0.3926 - val_loss: 2.2213 - val_main_output_loss: 2.2213 - val_main_output_acc: 0.4047\n",
      "Epoch 5/50\n",
      "33499/33499 [==============================] - 4s 108us/step - loss: 2.1666 - main_output_loss: 2.1669 - main_output_acc: 0.4128 - val_loss: 2.2550 - val_main_output_loss: 2.2550 - val_main_output_acc: 0.3638\n",
      "Epoch 6/50\n",
      "33499/33499 [==============================] - 4s 112us/step - loss: 2.0830 - main_output_loss: 2.0830 - main_output_acc: 0.4274 - val_loss: 2.3862 - val_main_output_loss: 2.3862 - val_main_output_acc: 0.3244\n",
      "Epoch 7/50\n",
      "33499/33499 [==============================] - 4s 112us/step - loss: 2.0024 - main_output_loss: 2.0025 - main_output_acc: 0.4443 - val_loss: 2.2084 - val_main_output_loss: 2.2084 - val_main_output_acc: 0.3902\n",
      "Epoch 8/50\n",
      "33499/33499 [==============================] - 4s 110us/step - loss: 1.9281 - main_output_loss: 1.9282 - main_output_acc: 0.4639 - val_loss: 1.9991 - val_main_output_loss: 1.9990 - val_main_output_acc: 0.4455\n",
      "Epoch 9/50\n",
      "33499/33499 [==============================] - 4s 111us/step - loss: 1.8543 - main_output_loss: 1.8542 - main_output_acc: 0.4801 - val_loss: 1.8856 - val_main_output_loss: 1.8856 - val_main_output_acc: 0.4595\n",
      "Epoch 10/50\n",
      "33499/33499 [==============================] - 4s 110us/step - loss: 1.7822 - main_output_loss: 1.7821 - main_output_acc: 0.5055 - val_loss: 1.7856 - val_main_output_loss: 1.7856 - val_main_output_acc: 0.4947\n",
      "Epoch 11/50\n",
      "33499/33499 [==============================] - 4s 111us/step - loss: 1.7161 - main_output_loss: 1.7162 - main_output_acc: 0.5249 - val_loss: 2.0654 - val_main_output_loss: 2.0654 - val_main_output_acc: 0.4154\n",
      "Epoch 12/50\n",
      "33499/33499 [==============================] - 4s 112us/step - loss: 1.6391 - main_output_loss: 1.6392 - main_output_acc: 0.5459 - val_loss: 1.8464 - val_main_output_loss: 1.8464 - val_main_output_acc: 0.4705\n",
      "Epoch 13/50\n",
      "33499/33499 [==============================] - 4s 111us/step - loss: 1.5675 - main_output_loss: 1.5674 - main_output_acc: 0.5668 - val_loss: 2.0493 - val_main_output_loss: 2.0492 - val_main_output_acc: 0.4230\n",
      "Epoch 14/50\n",
      "33499/33499 [==============================] - 4s 109us/step - loss: 1.5109 - main_output_loss: 1.5109 - main_output_acc: 0.5803 - val_loss: 1.6174 - val_main_output_loss: 1.6174 - val_main_output_acc: 0.5426\n",
      "Epoch 15/50\n",
      "33499/33499 [==============================] - 4s 111us/step - loss: 1.4544 - main_output_loss: 1.4545 - main_output_acc: 0.5917 - val_loss: 1.4792 - val_main_output_loss: 1.4792 - val_main_output_acc: 0.5921\n",
      "Epoch 16/50\n",
      "33499/33499 [==============================] - 4s 114us/step - loss: 1.4079 - main_output_loss: 1.4079 - main_output_acc: 0.6016 - val_loss: 1.6891 - val_main_output_loss: 1.6890 - val_main_output_acc: 0.5122\n",
      "Epoch 17/50\n",
      "33499/33499 [==============================] - 4s 112us/step - loss: 1.3637 - main_output_loss: 1.3640 - main_output_acc: 0.6146 - val_loss: 1.4886 - val_main_output_loss: 1.4886 - val_main_output_acc: 0.5977\n",
      "Epoch 18/50\n",
      "33499/33499 [==============================] - 4s 112us/step - loss: 1.3299 - main_output_loss: 1.3298 - main_output_acc: 0.6236 - val_loss: 1.6119 - val_main_output_loss: 1.6119 - val_main_output_acc: 0.5495\n",
      "Epoch 19/50\n",
      "33499/33499 [==============================] - 4s 113us/step - loss: 1.2890 - main_output_loss: 1.2891 - main_output_acc: 0.6367 - val_loss: 1.5858 - val_main_output_loss: 1.5857 - val_main_output_acc: 0.5379\n",
      "Epoch 20/50\n",
      "33499/33499 [==============================] - 4s 113us/step - loss: 1.2578 - main_output_loss: 1.2579 - main_output_acc: 0.6443 - val_loss: 1.5405 - val_main_output_loss: 1.5404 - val_main_output_acc: 0.5527\n",
      "Epoch 21/50\n",
      "33499/33499 [==============================] - 4s 112us/step - loss: 1.2278 - main_output_loss: 1.2278 - main_output_acc: 0.6512 - val_loss: 1.5978 - val_main_output_loss: 1.5977 - val_main_output_acc: 0.5500\n",
      "Epoch 22/50\n",
      "33499/33499 [==============================] - 4s 116us/step - loss: 1.1990 - main_output_loss: 1.1990 - main_output_acc: 0.6599 - val_loss: 1.5270 - val_main_output_loss: 1.5269 - val_main_output_acc: 0.5601\n",
      "Epoch 23/50\n",
      "33499/33499 [==============================] - 4s 122us/step - loss: 1.1707 - main_output_loss: 1.1706 - main_output_acc: 0.6643 - val_loss: 1.4145 - val_main_output_loss: 1.4145 - val_main_output_acc: 0.5925\n",
      "Epoch 24/50\n",
      "33499/33499 [==============================] - 4s 111us/step - loss: 1.1450 - main_output_loss: 1.1448 - main_output_acc: 0.6724 - val_loss: 1.2017 - val_main_output_loss: 1.2016 - val_main_output_acc: 0.6869\n",
      "Epoch 25/50\n",
      "33499/33499 [==============================] - 4s 111us/step - loss: 1.1205 - main_output_loss: 1.1206 - main_output_acc: 0.6788 - val_loss: 1.4467 - val_main_output_loss: 1.4466 - val_main_output_acc: 0.5876\n",
      "Epoch 26/50\n",
      "33499/33499 [==============================] - 4s 110us/step - loss: 1.1017 - main_output_loss: 1.1017 - main_output_acc: 0.6839 - val_loss: 1.4066 - val_main_output_loss: 1.4065 - val_main_output_acc: 0.5993\n",
      "Epoch 27/50\n",
      "33499/33499 [==============================] - 4s 112us/step - loss: 1.0817 - main_output_loss: 1.0818 - main_output_acc: 0.6879 - val_loss: 1.3017 - val_main_output_loss: 1.3017 - val_main_output_acc: 0.6233\n",
      "Epoch 28/50\n",
      "33499/33499 [==============================] - 4s 113us/step - loss: 1.0639 - main_output_loss: 1.0642 - main_output_acc: 0.6946 - val_loss: 1.2131 - val_main_output_loss: 1.2130 - val_main_output_acc: 0.6678\n",
      "Epoch 29/50\n",
      "33499/33499 [==============================] - 4s 111us/step - loss: 1.0502 - main_output_loss: 1.0502 - main_output_acc: 0.6964 - val_loss: 1.2990 - val_main_output_loss: 1.2990 - val_main_output_acc: 0.6272\n",
      "Epoch 30/50\n",
      "33499/33499 [==============================] - 4s 112us/step - loss: 1.0267 - main_output_loss: 1.0267 - main_output_acc: 0.7020 - val_loss: 1.2195 - val_main_output_loss: 1.2194 - val_main_output_acc: 0.6548\n",
      "Epoch 31/50\n",
      "33499/33499 [==============================] - 4s 111us/step - loss: 1.0158 - main_output_loss: 1.0158 - main_output_acc: 0.7053 - val_loss: 1.0512 - val_main_output_loss: 1.0511 - val_main_output_acc: 0.7145\n",
      "Epoch 32/50\n",
      "33499/33499 [==============================] - 4s 113us/step - loss: 1.0049 - main_output_loss: 1.0048 - main_output_acc: 0.7090 - val_loss: 1.1403 - val_main_output_loss: 1.1402 - val_main_output_acc: 0.6819\n",
      "Epoch 33/50\n",
      "33499/33499 [==============================] - 4s 114us/step - loss: 0.9800 - main_output_loss: 0.9801 - main_output_acc: 0.7138 - val_loss: 1.1296 - val_main_output_loss: 1.1295 - val_main_output_acc: 0.6810\n",
      "Epoch 34/50\n",
      "33499/33499 [==============================] - 4s 111us/step - loss: 0.9691 - main_output_loss: 0.9692 - main_output_acc: 0.7190 - val_loss: 1.8640 - val_main_output_loss: 1.8640 - val_main_output_acc: 0.4781\n",
      "Epoch 35/50\n",
      "33499/33499 [==============================] - 4s 114us/step - loss: 0.9646 - main_output_loss: 0.9647 - main_output_acc: 0.7196 - val_loss: 1.2316 - val_main_output_loss: 1.2315 - val_main_output_acc: 0.6419\n",
      "Epoch 36/50\n",
      "33499/33499 [==============================] - 4s 114us/step - loss: 0.9402 - main_output_loss: 0.9402 - main_output_acc: 0.7258 - val_loss: 1.0913 - val_main_output_loss: 1.0912 - val_main_output_acc: 0.6874\n",
      "Epoch 37/50\n",
      "33499/33499 [==============================] - 4s 114us/step - loss: 0.9331 - main_output_loss: 0.9332 - main_output_acc: 0.7286 - val_loss: 1.2584 - val_main_output_loss: 1.2584 - val_main_output_acc: 0.6384\n",
      "Epoch 38/50\n",
      "33499/33499 [==============================] - 4s 113us/step - loss: 0.9185 - main_output_loss: 0.9187 - main_output_acc: 0.7323 - val_loss: 1.3036 - val_main_output_loss: 1.3035 - val_main_output_acc: 0.6208\n",
      "Epoch 39/50\n",
      "33499/33499 [==============================] - 4s 113us/step - loss: 0.9125 - main_output_loss: 0.9125 - main_output_acc: 0.7313 - val_loss: 1.0332 - val_main_output_loss: 1.0331 - val_main_output_acc: 0.7164\n",
      "Epoch 40/50\n",
      "33499/33499 [==============================] - 4s 114us/step - loss: 0.9030 - main_output_loss: 0.9031 - main_output_acc: 0.7359 - val_loss: 1.0291 - val_main_output_loss: 1.0291 - val_main_output_acc: 0.7129\n",
      "Epoch 41/50\n",
      "33499/33499 [==============================] - 4s 113us/step - loss: 0.8945 - main_output_loss: 0.8945 - main_output_acc: 0.7361 - val_loss: 0.9459 - val_main_output_loss: 0.9459 - val_main_output_acc: 0.7346\n",
      "Epoch 42/50\n",
      "33499/33499 [==============================] - 4s 112us/step - loss: 0.8844 - main_output_loss: 0.8845 - main_output_acc: 0.7381 - val_loss: 1.0046 - val_main_output_loss: 1.0045 - val_main_output_acc: 0.7182\n",
      "Epoch 43/50\n",
      "33499/33499 [==============================] - 4s 114us/step - loss: 0.8682 - main_output_loss: 0.8683 - main_output_acc: 0.7411 - val_loss: 0.9239 - val_main_output_loss: 0.9238 - val_main_output_acc: 0.7442\n",
      "Epoch 44/50\n",
      "33499/33499 [==============================] - 4s 116us/step - loss: 0.8653 - main_output_loss: 0.8654 - main_output_acc: 0.7464 - val_loss: 1.0742 - val_main_output_loss: 1.0742 - val_main_output_acc: 0.6960\n",
      "Epoch 45/50\n",
      "33499/33499 [==============================] - 4s 113us/step - loss: 0.8464 - main_output_loss: 0.8466 - main_output_acc: 0.7498 - val_loss: 1.5226 - val_main_output_loss: 1.5225 - val_main_output_acc: 0.5550\n",
      "Epoch 46/50\n",
      "33499/33499 [==============================] - 4s 114us/step - loss: 0.8516 - main_output_loss: 0.8515 - main_output_acc: 0.7477 - val_loss: 0.9246 - val_main_output_loss: 0.9245 - val_main_output_acc: 0.7418\n",
      "Epoch 47/50\n",
      "33499/33499 [==============================] - 4s 114us/step - loss: 0.8340 - main_output_loss: 0.8340 - main_output_acc: 0.7517 - val_loss: 1.3300 - val_main_output_loss: 1.3298 - val_main_output_acc: 0.6285\n",
      "Epoch 48/50\n",
      "33499/33499 [==============================] - 4s 113us/step - loss: 0.8266 - main_output_loss: 0.8269 - main_output_acc: 0.7519 - val_loss: 0.8988 - val_main_output_loss: 0.8987 - val_main_output_acc: 0.7488\n",
      "Epoch 49/50\n",
      "33499/33499 [==============================] - 4s 114us/step - loss: 0.8264 - main_output_loss: 0.8264 - main_output_acc: 0.7529 - val_loss: 1.0095 - val_main_output_loss: 1.0094 - val_main_output_acc: 0.7116\n",
      "Epoch 50/50\n",
      "33499/33499 [==============================] - 4s 115us/step - loss: 0.8171 - main_output_loss: 0.8171 - main_output_acc: 0.7570 - val_loss: 0.9966 - val_main_output_loss: 0.9965 - val_main_output_acc: 0.7205\n",
      "Training complete \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. train model\n",
    "bat_size = 128\n",
    "max_epochs = 50\n",
    "print(\"\\nStarting training \")\n",
    "#model.fit(np.array(train_x), train_y, epochs=max_epochs,\n",
    "#  batch_size=bat_size, shuffle=True, verbose=1) \n",
    "model.fit(np.array(train_x), train_y, epochs=max_epochs,\n",
    "  batch_size=bat_size, shuffle=True, verbose=1, validation_data=(np.array(test_x), test_y)) \n",
    "print(\"Training complete \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.array(fewclasses['text'])\n",
    "y2 = np.array(pd.factorize(fewclasses['author'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output features missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to features.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model_fewshot_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = []\n",
    "\n",
    "for i in range(5):\n",
    "    x3.append(np.random.choice(x2[y2==i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1000)\n",
      "(5206,)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(x2.shape)\n",
    "print(x3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = np.array(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(x3)\n",
    "test_x3 = tokenizer.texts_to_sequences(x2)\n",
    "\n",
    "\n",
    "\n",
    "max_review_len = 1000\n",
    "train_x = K.preprocessing.sequence.pad_sequences(sequences,\n",
    "  truncating='pre', padding='pre', maxlen=max_review_len)  # pad and chop!\n",
    "test_x = K.preprocessing.sequence.pad_sequences(test_x3,\n",
    "  truncating='pre', padding='pre', maxlen=max_review_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, _ = model.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, _ = model.predict(np.array(test_x3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.expand_dims(train_features, axis=0)\n",
    "test_features = np.expand_dims(test_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 40)\n",
      "(5206, 1, 40)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif = train_features - test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5206, 5, 40)\n"
     ]
    }
   ],
   "source": [
    "print(dif.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5206,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "distance = np.linalg.norm(dif, axis=2)\n",
    "predictions = np.argmin(distance, axis=1)\n",
    "'''index = np.argpartition(distance, self.k)\n",
    "values = index[:self.k]\n",
    "unique, counts = np.unique(self.y[values], return_counts=True)\n",
    "prediction = unique[np.argmax(counts)]'''\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19996158278908951"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions == y2)/5206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_fewshot_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Few Shot Implementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:author_classification]",
   "language": "python",
   "name": "conda-env-author_classification-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
