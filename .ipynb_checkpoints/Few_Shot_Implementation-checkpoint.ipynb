{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Few Shot Implementation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3Gqixx8CfPG",
        "colab_type": "code",
        "outputId": "66c3bef4-dafd-45a8-9ba6-327fcb66a75c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GImGbTN3HFYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import keras as K\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlXvfS9UI6LV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJP09jSpHTta",
        "colab_type": "code",
        "outputId": "31d5fd57-1e2f-429d-e7ba-8bd4b679bd3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "tf.set_random_seed(1)\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Author ID/Gungor_2018_VictorianAuthorAttribution_data-train.csv\")\n",
        "\n",
        "df['author'].unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  2,  3,  4,  6,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
              "       20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37,\n",
              "       38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 50])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4KUUtRQJUAf",
        "colab_type": "code",
        "outputId": "caf6c9f3-11fb-42fd-c6b0-383bdaed8d1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "p = np.random.permutation(df['author'].unique())\n",
        "trainclass = p[:40]\n",
        "fewclass = p[40:]\n",
        "\n",
        "print(randvals)\n",
        "\n",
        "trainingclasses = df[df['author'].isin(trainclass)]\n",
        "\n",
        "fewclasses = df[df['author'].isin(fewclass)]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[14 40 44  5 17 35 18 34 32  9 15 42 25 43 10 19 39 16 22 28 31 29  4 20\n",
            " 26 33 30  8 38  7 11 36 27 12  2 37 21  6 41  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVgUDABPELK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.datasets import fetch_mldata\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import one_hot, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "\n",
        "from keras.layers import *\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
        "\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdNueNb1Ex0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.array(trainingclasses['text'])\n",
        "y = np.array(pd.factorize(trainingclasses['author'])[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n43q4SkwEeJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_words = 20000\n",
        "                \n",
        "tokenizer = Tokenizer(20000)\n",
        "tokenizer.fit_on_texts(x)\n",
        "sequences = tokenizer.texts_to_sequences(x)\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(sequences, y, test_size=0.33, random_state=42)\n",
        "max_review_len = 1000\n",
        "train_x = K.preprocessing.sequence.pad_sequences(train_x,\n",
        "  truncating='pre', padding='pre', maxlen=max_review_len)  # pad and chop!\n",
        "test_x = K.preprocessing.sequence.pad_sequences(test_x,\n",
        "  truncating='pre', padding='pre', maxlen=max_review_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbUpfaezHVnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x, test_x, train_y, test_y = train_test_split(sequences,\n",
        "                                                    y, test_size=0.33,\n",
        "                                                    random_state=42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viyCWVWjEXbX",
        "colab_type": "code",
        "outputId": "df1facd0-35dd-44a3-a0ec-ec56c0074e3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "input1 = Input(shape=(1000,))\n",
        "\n",
        "x = Embedding(20000, 8, input_length=1000)(input1)\n",
        "\n",
        "x = Conv1D(16, 3, strides=1)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.1)(x)\n",
        "\n",
        "x = Conv1D(16, 3, strides=2)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.1)(x)\n",
        "\n",
        "x = Conv1D(16, 3, strides=3)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.1)(x)\n",
        "\n",
        "x = Conv1D(16, 3, strides=3)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.1)(x)\n",
        "\n",
        "x = Conv1D(16, 3, strides=3)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "feats = Flatten(name='features')(x)\n",
        "\n",
        "x = Dense(40)(feats)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('softmax', name='main_output')(x)\n",
        "\n",
        "model = Model(inputs=input1, outputs=[x, feats])\n",
        "\n",
        "\n",
        "opt = K.optimizers.Adam(lr=0.001)\n",
        "model.compile(opt, loss={'main_output': 'sparse_categorical_crossentropy'}, metrics=['acc'])\n",
        "\n",
        "\n",
        "\n",
        "print(model.summary()) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 1000, 8)           160000    \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 998, 16)           400       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 998, 16)           64        \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 998, 16)           0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 998, 16)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 498, 16)           784       \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 498, 16)           64        \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 498, 16)           0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 498, 16)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_8 (Conv1D)            (None, 166, 16)           784       \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 166, 16)           64        \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 166, 16)           0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 166, 16)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 55, 16)            784       \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 55, 16)            64        \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 55, 16)            0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 55, 16)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_10 (Conv1D)           (None, 18, 16)            784       \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 18, 16)            64        \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 18, 16)            0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 18, 16)            0         \n",
            "_________________________________________________________________\n",
            "features (Flatten)           (None, 288)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 40)                11560     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 40)                160       \n",
            "_________________________________________________________________\n",
            "main_output (Activation)     (None, 40)                0         \n",
            "=================================================================\n",
            "Total params: 175,576\n",
            "Trainable params: 175,336\n",
            "Non-trainable params: 240\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: Output \"features\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"features\" during training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV-RM9vC1nqM",
        "colab_type": "code",
        "outputId": "15136e5d-5b7a-4eec-e7db-6f61d229b5f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        }
      },
      "source": [
        "# 3. train model\n",
        "bat_size = 128\n",
        "max_epochs = 50\n",
        "print(\"\\nStarting training \")\n",
        "model.fit(train_x, train_y, epochs=max_epochs,\n",
        "  batch_size=bat_size, shuffle=True, verbose=1, validation_data=(test_x, test_y)) \n",
        "print(\"Training complete \\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-a7e07f0fd8da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting training \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model.fit(train_x, train_y, epochs=max_epochs,\n\u001b[0;32m----> 5\u001b[0;31m   batch_size=bat_size, shuffle=True, verbose=1, validation_data=(test_x, test_y)) \n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training complete \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             raise ValueError(\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 32476 arrays: [array([[2989],\n       [  20],\n       [  21],\n       [  63],\n       [1898],\n       [1784],\n       [ 127],\n       [  49],\n       [  21],\n       [  63],\n       [ 549],\n       [  78],\n       [ 103],\n    ..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsU3T03PGORG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}