{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GImGbTN3HFYD",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras as K\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "ZJP09jSpHTta",
    "outputId": "31d5fd57-1e2f-429d-e7ba-8bd4b679bd3e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  6,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "       20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37,\n",
       "       38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 50])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "print(tf.__version__)\n",
    "\n",
    "df = pd.read_csv(\"Gungor_2018_VictorianAuthorAttribution_data-train.csv\")\n",
    "\n",
    "df['author'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Y4KUUtRQJUAf",
    "outputId": "caf6c9f3-11fb-42fd-c6b0-383bdaed8d1a"
   },
   "outputs": [],
   "source": [
    "# p = np.random.permutation(df['author'].unique())\n",
    "# trainclass = p[:40]\n",
    "# fewclass = p[40:]\n",
    "\n",
    "# trainingclasses = df[df['author'].isin(trainclass)]\n",
    "\n",
    "# fewclasses = df[df['author'].isin(fewclass)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingclasses = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53678"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainingclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVgUDABPELK4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 64\n",
    "x = []\n",
    "y = np.empty((len(trainingclasses) * (1000 // seq_len)), dtype=np.int64)\n",
    "labels, _ = pd.factorize(trainingclasses['author'])\n",
    "trainingclasses['author_codes'] = labels\n",
    "row = 0\n",
    "for text, author in trainingclasses[['text', 'author_codes']].values:\n",
    "    words = text.split()\n",
    "    for j in range(1000//seq_len):\n",
    "        x.append(\" \".join(words[j*seq_len:j*seq_len+seq_len]))\n",
    "        y[row] = author\n",
    "        row+=1\n",
    "\n",
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LdNueNb1Ex0H"
   },
   "outputs": [],
   "source": [
    "# x = np.array(trainingclasses['text'])\n",
    "# y = np.array(pd.factorize(trainingclasses['author'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my friend with thanks that i should at once put it in shape for my readers i said i should make a few alterations in it for the sake of dramatic interest but in the main would follow the lines he had given me it would spoil my romance were i to answer on this page the question that must be uppermost in the'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n43q4SkwEeJp"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "max_words = 20000\n",
    "                \n",
    "tokenizer = Tokenizer(20000)\n",
    "tokenizer.fit_on_texts(x)\n",
    "sequences = tokenizer.texts_to_sequences(x)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(sequences, y, test_size=0.33, random_state=42)\n",
    "max_review_len = 64\n",
    "train_x = K.preprocessing.sequence.pad_sequences(train_x,\n",
    "  truncating='pre', padding='pre', maxlen=max_review_len)  # pad and chop!\n",
    "test_x = K.preprocessing.sequence.pad_sequences(test_x,\n",
    "  truncating='pre', padding='pre', maxlen=max_review_len)\n",
    "\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle)\n",
    "    \n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('./GLOVE_DIR', 'glove.6B.50d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 50))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            50,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_review_len,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dbUpfaezHVnT"
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(sequences,\n",
    "                                                    y, test_size=0.10,\n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "viyCWVWjEXbX",
    "outputId": "df1facd0-35dd-44a3-a0ec-ec56c0074e3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 64, 50)            500050    \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 62, 128)           19328     \n",
      "_________________________________________________________________\n",
      "activation_75 (Activation)   (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 60, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_76 (Activation)   (None, 60, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 60, 128)           0         \n",
      "_________________________________________________________________\n",
      "features (Flatten)           (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               3932672   \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 45)                23085     \n",
      "_________________________________________________________________\n",
      "main_output (Activation)     (None, 45)                0         \n",
      "=================================================================\n",
      "Total params: 4,524,415\n",
      "Trainable params: 4,024,365\n",
      "Non-trainable params: 500,050\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input1 = Input(shape=(64,))\n",
    "\n",
    "x = embedding_layer(input1)\n",
    "\n",
    "x = Conv1D(128, 3, strides=1)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Conv1D(128, 3, strides=1)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# x = Conv1D(64, 3, strides=1)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "\n",
    "# x = Conv1D(64, 3, strides=1)(x)\n",
    "# # x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "\n",
    "# x = Conv1D(16, 3, strides=3)(x)\n",
    "# # x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "\n",
    "feats = Flatten(name='features')(x)\n",
    "\n",
    "x = Dense(512)(feats)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(45)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "x = Activation('softmax', name='main_output')(x)\n",
    "\n",
    "model = Model(inputs=input1, outputs=[x, feats])\n",
    "\n",
    "\n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output features missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to features.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "opt = K.optimizers.Adam(lr=0.001)\n",
    "model.compile(opt, loss={'main_output': 'sparse_categorical_crossentropy'}, metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "colab_type": "code",
    "id": "JV-RM9vC1nqM",
    "outputId": "15136e5d-5b7a-4eec-e7db-6f61d229b5f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training \n",
      "Train on 724653 samples, validate on 80517 samples\n",
      "Epoch 1/50\n",
      "724653/724653 [==============================] - 27s 37us/step - loss: 3.0860 - main_output_loss: 3.0858 - main_output_acc: 0.2136 - val_loss: 2.6842 - val_main_output_loss: 2.6837 - val_main_output_acc: 0.2910\n",
      "Epoch 2/50\n",
      "724653/724653 [==============================] - 25s 35us/step - loss: 2.5903 - main_output_loss: 2.5902 - main_output_acc: 0.3177 - val_loss: 2.5006 - val_main_output_loss: 2.5003 - val_main_output_acc: 0.3361\n",
      "Epoch 3/50\n",
      "724653/724653 [==============================] - 26s 36us/step - loss: 2.4115 - main_output_loss: 2.4115 - main_output_acc: 0.3596 - val_loss: 2.3477 - val_main_output_loss: 2.3475 - val_main_output_acc: 0.3737\n",
      "Epoch 4/50\n",
      "724653/724653 [==============================] - 26s 36us/step - loss: 2.3017 - main_output_loss: 2.3018 - main_output_acc: 0.3854 - val_loss: 2.2527 - val_main_output_loss: 2.2525 - val_main_output_acc: 0.4031\n",
      "Epoch 5/50\n",
      "724653/724653 [==============================] - 26s 36us/step - loss: 2.2281 - main_output_loss: 2.2281 - main_output_acc: 0.4034 - val_loss: 2.2270 - val_main_output_loss: 2.2270 - val_main_output_acc: 0.4106\n",
      "Epoch 6/50\n",
      "724653/724653 [==============================] - 25s 35us/step - loss: 2.1773 - main_output_loss: 2.1772 - main_output_acc: 0.4149 - val_loss: 2.1886 - val_main_output_loss: 2.1882 - val_main_output_acc: 0.4207\n",
      "Epoch 7/50\n",
      "724653/724653 [==============================] - 25s 35us/step - loss: 2.1392 - main_output_loss: 2.1392 - main_output_acc: 0.4244 - val_loss: 2.1755 - val_main_output_loss: 2.1755 - val_main_output_acc: 0.4202\n",
      "Epoch 8/50\n",
      "724653/724653 [==============================] - 25s 35us/step - loss: 2.1110 - main_output_loss: 2.1110 - main_output_acc: 0.4309 - val_loss: 2.1720 - val_main_output_loss: 2.1724 - val_main_output_acc: 0.4234\n",
      "Epoch 9/50\n",
      "724653/724653 [==============================] - 26s 36us/step - loss: 2.0870 - main_output_loss: 2.0871 - main_output_acc: 0.4362 - val_loss: 2.1446 - val_main_output_loss: 2.1450 - val_main_output_acc: 0.4299\n",
      "Epoch 10/50\n",
      "724653/724653 [==============================] - 27s 37us/step - loss: 2.0672 - main_output_loss: 2.0673 - main_output_acc: 0.4407 - val_loss: 2.1352 - val_main_output_loss: 2.1358 - val_main_output_acc: 0.4360\n",
      "Epoch 11/50\n",
      "724653/724653 [==============================] - 27s 37us/step - loss: 2.0502 - main_output_loss: 2.0502 - main_output_acc: 0.4447 - val_loss: 2.1402 - val_main_output_loss: 2.1407 - val_main_output_acc: 0.4311\n",
      "Epoch 12/50\n",
      "724653/724653 [==============================] - 25s 35us/step - loss: 2.0377 - main_output_loss: 2.0377 - main_output_acc: 0.4476 - val_loss: 2.1336 - val_main_output_loss: 2.1340 - val_main_output_acc: 0.4321\n",
      "Epoch 13/50\n",
      "724653/724653 [==============================] - 25s 35us/step - loss: 2.0255 - main_output_loss: 2.0255 - main_output_acc: 0.4502 - val_loss: 2.1414 - val_main_output_loss: 2.1419 - val_main_output_acc: 0.4292\n",
      "Epoch 14/50\n",
      "724653/724653 [==============================] - 26s 36us/step - loss: 2.0162 - main_output_loss: 2.0162 - main_output_acc: 0.4527 - val_loss: 2.1016 - val_main_output_loss: 2.1022 - val_main_output_acc: 0.4415\n",
      "Epoch 15/50\n",
      "724653/724653 [==============================] - 26s 36us/step - loss: 2.0056 - main_output_loss: 2.0056 - main_output_acc: 0.4556 - val_loss: 2.0897 - val_main_output_loss: 2.0901 - val_main_output_acc: 0.4477\n",
      "Epoch 16/50\n",
      "724653/724653 [==============================] - 25s 35us/step - loss: 1.9965 - main_output_loss: 1.9966 - main_output_acc: 0.4580 - val_loss: 2.0871 - val_main_output_loss: 2.0876 - val_main_output_acc: 0.4472\n",
      "Epoch 17/50\n",
      "724653/724653 [==============================] - 25s 35us/step - loss: 1.9904 - main_output_loss: 1.9904 - main_output_acc: 0.4592 - val_loss: 2.0941 - val_main_output_loss: 2.0944 - val_main_output_acc: 0.4442\n",
      "Epoch 18/50\n",
      "724653/724653 [==============================] - 26s 35us/step - loss: 1.9838 - main_output_loss: 1.9838 - main_output_acc: 0.4604 - val_loss: 2.0756 - val_main_output_loss: 2.0762 - val_main_output_acc: 0.4496\n",
      "Epoch 19/50\n",
      "724653/724653 [==============================] - 26s 35us/step - loss: 1.9766 - main_output_loss: 1.9767 - main_output_acc: 0.4624 - val_loss: 2.0941 - val_main_output_loss: 2.0944 - val_main_output_acc: 0.4445\n",
      "Epoch 20/50\n",
      "724653/724653 [==============================] - 25s 35us/step - loss: 1.9709 - main_output_loss: 1.9708 - main_output_acc: 0.4636 - val_loss: 2.0723 - val_main_output_loss: 2.0724 - val_main_output_acc: 0.4504\n",
      "Epoch 21/50\n",
      "724653/724653 [==============================] - 26s 35us/step - loss: 1.9651 - main_output_loss: 1.9651 - main_output_acc: 0.4642 - val_loss: 2.0666 - val_main_output_loss: 2.0669 - val_main_output_acc: 0.4526\n",
      "Epoch 22/50\n",
      "724653/724653 [==============================] - 26s 35us/step - loss: 1.9619 - main_output_loss: 1.9619 - main_output_acc: 0.4655 - val_loss: 2.0720 - val_main_output_loss: 2.0722 - val_main_output_acc: 0.4495\n",
      "Epoch 23/50\n",
      "724653/724653 [==============================] - 26s 35us/step - loss: 1.9548 - main_output_loss: 1.9548 - main_output_acc: 0.4675 - val_loss: 2.0748 - val_main_output_loss: 2.0744 - val_main_output_acc: 0.4497\n",
      "Epoch 24/50\n",
      "724653/724653 [==============================] - 26s 35us/step - loss: 1.9527 - main_output_loss: 1.9527 - main_output_acc: 0.4678 - val_loss: 2.0627 - val_main_output_loss: 2.0629 - val_main_output_acc: 0.4529\n",
      "Epoch 25/50\n",
      "724653/724653 [==============================] - 26s 35us/step - loss: 1.9465 - main_output_loss: 1.9466 - main_output_acc: 0.4699 - val_loss: 2.0595 - val_main_output_loss: 2.0596 - val_main_output_acc: 0.4521\n",
      "Epoch 26/50\n",
      "724653/724653 [==============================] - 26s 35us/step - loss: 1.9444 - main_output_loss: 1.9443 - main_output_acc: 0.4701 - val_loss: 2.0532 - val_main_output_loss: 2.0534 - val_main_output_acc: 0.4555\n",
      "Epoch 27/50\n",
      "724653/724653 [==============================] - 26s 35us/step - loss: 1.9390 - main_output_loss: 1.9390 - main_output_acc: 0.4712 - val_loss: 2.0571 - val_main_output_loss: 2.0572 - val_main_output_acc: 0.4550\n",
      "Epoch 28/50\n",
      "724653/724653 [==============================] - 26s 36us/step - loss: 1.9366 - main_output_loss: 1.9366 - main_output_acc: 0.4714 - val_loss: 2.0690 - val_main_output_loss: 2.0691 - val_main_output_acc: 0.4543\n",
      "Epoch 29/50\n",
      "724653/724653 [==============================] - 26s 35us/step - loss: 1.9355 - main_output_loss: 1.9356 - main_output_acc: 0.4712 - val_loss: 2.0547 - val_main_output_loss: 2.0546 - val_main_output_acc: 0.4547\n",
      "Epoch 30/50\n",
      "724653/724653 [==============================] - 26s 35us/step - loss: 1.9303 - main_output_loss: 1.9304 - main_output_acc: 0.4734 - val_loss: 2.0583 - val_main_output_loss: 2.0585 - val_main_output_acc: 0.4585\n",
      "Epoch 31/50\n",
      "724653/724653 [==============================] - 26s 36us/step - loss: 1.9274 - main_output_loss: 1.9274 - main_output_acc: 0.4739 - val_loss: 2.0637 - val_main_output_loss: 2.0638 - val_main_output_acc: 0.4540\n",
      "Epoch 32/50\n",
      "724653/724653 [==============================] - 26s 36us/step - loss: 1.9255 - main_output_loss: 1.9254 - main_output_acc: 0.4747 - val_loss: 2.0719 - val_main_output_loss: 2.0721 - val_main_output_acc: 0.4526\n",
      "Epoch 33/50\n",
      "724653/724653 [==============================] - 25s 35us/step - loss: 1.9213 - main_output_loss: 1.9213 - main_output_acc: 0.4753 - val_loss: 2.0466 - val_main_output_loss: 2.0463 - val_main_output_acc: 0.4584\n",
      "Epoch 34/50\n",
      "724653/724653 [==============================] - 26s 35us/step - loss: 1.9204 - main_output_loss: 1.9205 - main_output_acc: 0.4750 - val_loss: 2.0449 - val_main_output_loss: 2.0450 - val_main_output_acc: 0.4587\n",
      "Epoch 35/50\n",
      " 28160/724653 [>.............................] - ETA: 24s - loss: 1.8510 - main_output_loss: 1.8510 - main_output_acc: 0.4902"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-0d88124e4004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#  batch_size=bat_size, shuffle=True, verbose=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m model.fit(np.array(train_x), train_y, epochs=max_epochs,\n\u001b[0;32m----> 8\u001b[0;31m   batch_size=bat_size, shuffle=True, verbose=1, validation_data=(np.array(test_x), test_y)) \n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training complete \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/author_classification/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/author_classification/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/miniconda3/envs/author_classification/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3. train model\n",
    "bat_size = 512\n",
    "max_epochs = 50\n",
    "print(\"\\nStarting training \")\n",
    "#model.fit(np.array(train_x), train_y, epochs=max_epochs,\n",
    "#  batch_size=bat_size, shuffle=True, verbose=1) \n",
    "model.fit(np.array(train_x), train_y, epochs=max_epochs,\n",
    "  batch_size=bat_size, shuffle=True, verbose=1, validation_data=(np.array(test_x), test_y)) \n",
    "print(\"Training complete \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = sequences[:200*15]\n",
    "eval_ys = [y[i] for i in range(0,200*15,15)]\n",
    "predictions, _ = model.predict(np.array(evals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 45)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.reshape((,45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.array(fewclasses['text'])\n",
    "y2 = np.array(pd.factorize(fewclasses['author'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tokyotechies/miniconda3/envs/author_classification/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output features missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to features.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model_fewshot_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = []\n",
    "\n",
    "for i in range(5):\n",
    "    x3.append(np.random.choice(x2[y2==i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1000)\n",
      "(5206,)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(x2.shape)\n",
    "print(x3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = np.array(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(x3)\n",
    "test_x3 = tokenizer.texts_to_sequences(x2)\n",
    "\n",
    "\n",
    "\n",
    "max_review_len = 1000\n",
    "train_x = K.preprocessing.sequence.pad_sequences(sequences,\n",
    "  truncating='pre', padding='pre', maxlen=max_review_len)  # pad and chop!\n",
    "test_x = K.preprocessing.sequence.pad_sequences(test_x3,\n",
    "  truncating='pre', padding='pre', maxlen=max_review_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, _ = model.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, _ = model.predict(np.array(test_x3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.expand_dims(train_features, axis=0)\n",
    "test_features = np.expand_dims(test_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 40)\n",
      "(5206, 1, 40)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif = train_features - test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5206, 5, 40)\n"
     ]
    }
   ],
   "source": [
    "print(dif.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5206,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "distance = np.linalg.norm(dif, axis=2)\n",
    "predictions = np.argmin(distance, axis=1)\n",
    "'''index = np.argpartition(distance, self.k)\n",
    "values = index[:self.k]\n",
    "unique, counts = np.unique(self.y[values], return_counts=True)\n",
    "prediction = unique[np.argmax(counts)]'''\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19996158278908951"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions == y2)/5206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_fewshot_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Few Shot Implementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:author_classification]",
   "language": "python",
   "name": "conda-env-author_classification-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
